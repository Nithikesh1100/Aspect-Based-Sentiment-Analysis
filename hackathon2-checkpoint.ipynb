{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934af07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58733a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the correct Python paths for PySpark\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\NITHIKESH\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\NITHIKESH\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test Spark Session\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70d0ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- AcceptsInsurance: string (nullable = true)\n",
      " |    |-- AgesAllowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: string (nullable = true)\n",
      " |    |-- BYOB: string (nullable = true)\n",
      " |    |-- BYOBCorkage: string (nullable = true)\n",
      " |    |-- BestNights: string (nullable = true)\n",
      " |    |-- BikeParking: string (nullable = true)\n",
      " |    |-- BusinessAcceptsBitcoin: string (nullable = true)\n",
      " |    |-- BusinessAcceptsCreditCards: string (nullable = true)\n",
      " |    |-- BusinessParking: string (nullable = true)\n",
      " |    |-- ByAppointmentOnly: string (nullable = true)\n",
      " |    |-- Caters: string (nullable = true)\n",
      " |    |-- CoatCheck: string (nullable = true)\n",
      " |    |-- Corkage: string (nullable = true)\n",
      " |    |-- DietaryRestrictions: string (nullable = true)\n",
      " |    |-- DogsAllowed: string (nullable = true)\n",
      " |    |-- DriveThru: string (nullable = true)\n",
      " |    |-- GoodForDancing: string (nullable = true)\n",
      " |    |-- GoodForKids: string (nullable = true)\n",
      " |    |-- GoodForMeal: string (nullable = true)\n",
      " |    |-- HairSpecializesIn: string (nullable = true)\n",
      " |    |-- HappyHour: string (nullable = true)\n",
      " |    |-- HasTV: string (nullable = true)\n",
      " |    |-- Music: string (nullable = true)\n",
      " |    |-- NoiseLevel: string (nullable = true)\n",
      " |    |-- Open24Hours: string (nullable = true)\n",
      " |    |-- OutdoorSeating: string (nullable = true)\n",
      " |    |-- RestaurantsAttire: string (nullable = true)\n",
      " |    |-- RestaurantsCounterService: string (nullable = true)\n",
      " |    |-- RestaurantsDelivery: string (nullable = true)\n",
      " |    |-- RestaurantsGoodForGroups: string (nullable = true)\n",
      " |    |-- RestaurantsPriceRange2: string (nullable = true)\n",
      " |    |-- RestaurantsReservations: string (nullable = true)\n",
      " |    |-- RestaurantsTableService: string (nullable = true)\n",
      " |    |-- RestaurantsTakeOut: string (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- WheelchairAccessible: string (nullable = true)\n",
      " |    |-- WiFi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: string (nullable = true)\n",
      " |    |-- Monday: string (nullable = true)\n",
      " |    |-- Saturday: string (nullable = true)\n",
      " |    |-- Sunday: string (nullable = true)\n",
      " |    |-- Thursday: string (nullable = true)\n",
      " |    |-- Tuesday: string (nullable = true)\n",
      " |    |-- Wednesday: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150346"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business = spark.read.json(r\"C:\\Users\\NITHIKESH\\Downloads\\extracted\\dataset\\yelp_academic_dataset_business.json\")\n",
    "business.printSchema()\n",
    "business.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fc88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = spark.read.json(r\"C:\\Users\\NITHIKESH\\Downloads\\extracted\\dataset\\yelp_academic_dataset_review.json\")\n",
    "review.printSchema()\n",
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c242bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- compliment_count: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "908915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip = spark.read.json(r\"C:\\Users\\NITHIKESH\\Downloads\\extracted\\dataset\\yelp_academic_dataset_tip.json\")\n",
    "tip.printSchema()\n",
    "tip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716c2598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business=business.select(\"business_id\",\"name\",\"review_count\",\"is_open\")\n",
    "business.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a937d6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business = business.filter(business.is_open == 1)\n",
    "business.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9615aa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review=review.select('user_id' ,'business_id', 'stars','text')\n",
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f2a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             user_id|         business_id|            tip_text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|AGNUgVwnZUey3gcPC...|3uLgwr0qeCNMjKenH...|Avengers time wit...|\n",
      "|NBN4MgHP9D3cw--Sn...|QoezRbYQncpRqyrLH...|They have lots of...|\n",
      "|-copOvldyKh1qr-vz...|MYoRNLb5chwjQe3c_...|It's open even wh...|\n",
      "|FjMQVZjSqY8syIO-5...|hV-bABTK-glh5wj31...|Very decent fried...|\n",
      "|ld0AperBXk1h6Ubqm...|_uN0OudeJ3Zl_tf6n...|Appetizers.. plat...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip=tip.select('user_id' ,'business_id', 'text')\n",
    "tip = tip.withColumnRenamed(\"text\", \"tip_text\")\n",
    "tip.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92e321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "|         business_id|             user_id|stars|                text|                name|review_count|is_open|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "|XQfwVwDr-v0ZS3_Cb...|mh_-eMZ6K5RLWhZyI...|  3.0|If you decide to ...|Turning Point of ...|         169|      1|\n",
      "|YjUWPpI6HXG530lwP...|8g_iMtfSiwikVnbP2...|  3.0|Family diner. Had...|   Kettle Restaurant|          47|      1|\n",
      "|kxX2SOes4o-D3ZQBk...|_7bHUi9Uuf5__HHc_...|  5.0|Wow!  Yummy, diff...|               Zaika|         181|      1|\n",
      "|gmjsEdUsKpj9Xxu6p...|r3zeYsv1XFBRA4dJp...|  5.0|Loved this tour! ...|The Voodoo Bone L...|         359|      1|\n",
      "|B5XSoSG3SfvQGtKEG...|wSTuiTk-sKNdcFypr...|  3.0|This easter inste...|Los Padres Nation...|          13|      1|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = review.join(business, on=\"business_id\", how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d97e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- tip_text: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "|         business_id|             user_id|            tip_text|stars|                text|                name|review_count|is_open|\n",
      "+--------------------+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "|-0TffRSXXIlBYVbb5...|8e2bnPezIoW8ySZ0A...|Great food and Li...|  5.0|Great food and Li...|IndeBlue Modern I...|        1097|      1|\n",
      "|-0YvX6VK5S0u_hbQF...|T9tYtHUKNmJE1N6Ar...|Don't believe tho...|  1.0|Don't believe tho...| European Motorworks|           9|      1|\n",
      "|-0iIxySkp97WNlwK6...|cRANsQ5E_sxeQ0Aoo...|Get the hummus, i...|  4.0|I met a friend he...|Truckee Bagel Com...|         219|      1|\n",
      "|-0iIxySkp97WNlwK6...|lRRVRehFcudfbjY6y...|Jalapeño bagel is...|  5.0|I definitely had ...|Truckee Bagel Com...|         219|      1|\n",
      "|-0iIxySkp97WNlwK6...|lRRVRehFcudfbjY6y...|Cranberry is a li...|  5.0|I definitely had ...|Truckee Bagel Com...|         219|      1|\n",
      "+--------------------+--------------------+--------------------+-----+--------------------+--------------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform an inner join on 'business_id' and 'user_id'\n",
    "df_merged = tip.join(df, on=[\"business_id\", \"user_id\"], how=\"inner\")\n",
    "\n",
    "# Show the schema and result of the merged DataFrame\n",
    "df_merged.printSchema()\n",
    "df_merged.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aaef25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- tip_text: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df_merged.withColumn(\"full_text\", concat(col(\"text\"), lit(\" \"), col(\"tip_text\")))\n",
    "\n",
    "# Show the DataFrame to verify the concatenation\n",
    "# df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef272d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----------------+--------------------+\n",
      "|         business_id|             user_id|stars|             name|           full_text|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+\n",
      "|---kPU91CF4Lq2-Wl...|zmgsdGzOp08BWJZ2y...|  5.0|Frankie's Raw Bar|Awesome raw bar /...|\n",
      "|--LC8cIrALInl2vyo...|lVW0BD_ZkWVGDVwBu...|  4.0|   Studio G Salon|Kathy at Studio G...|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, col\n",
    "\n",
    "# Drop the specified columns\n",
    "df_dropped = df.drop(\"text\", \"tip_text\", \"is_open\", \"review_count\")\n",
    "\n",
    "# Group by 'business_id' and aggregate the remaining columns\n",
    "# Here we use 'first' to keep the first value in each group; adjust as needed\n",
    "df_grouped = df_dropped.groupBy(\"business_id\").agg(\n",
    "    first(\"user_id\").alias(\"user_id\"),\n",
    "    first(\"stars\").alias(\"stars\"),\n",
    "    first(\"name\").alias(\"name\"),\n",
    "    first(\"full_text\").alias(\"full_text\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_grouped.show(2)\n",
    "df_grouped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a20b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Food Quality': ['The food was good.'], 'Service': ['The service was ok!'], 'Ambiance': ['The ambiance was excellent and the cleanliness was spot on.'], 'Pricing': ['However, the price was a bit high.'], 'Cleanliness': ['The ambiance was excellent and the cleanliness was spot on.']}\n"
     ]
    }
   ],
   "source": [
    "# Define keywords for each aspect\n",
    "aspect_keywords = {\n",
    "    'Food Quality': ['food','taste', 'flavor', 'texture', 'freshness', 'quality', 'portion', 'ingredients', 'preparation', 'presentation', 'dish', 'meal', 'cuisine', 'gourmet', 'delicious', 'overcooked', 'undercooked'],\n",
    "    'Service': ['service', 'waitstaff', 'staff', 'server', 'attentiveness', 'friendliness', 'promptness', 'helpfulness', 'efficiency', 'customer service', 'support', 'response time', 'hospitality', 'professionalism'],\n",
    "    'Ambiance': ['ambiance', 'atmosphere', 'decor', 'setting', 'environment', 'lighting', 'music', 'comfort', 'vibe', 'aesthetic', 'interior', 'noise level', 'seating', 'cleanliness'],\n",
    "    'Pricing': ['price', 'cost', 'value', 'expensive', 'cheap', 'affordable', 'worth', 'rate', 'value for money', 'budget', 'discount', 'overpriced', 'underpriced', 'deal'],\n",
    "    'Cleanliness': ['cleanliness', 'hygiene', 'clean', 'neat', 'tidy', 'sanitation', 'spotless', 'messy', 'dirty', 'scrubbed', 'sterile', 'orderly', 'maintenance', 'restrooms']\n",
    "}\n",
    "\n",
    "# Updated function to handle multiple aspect keywords\n",
    "def extract_aspects(text, aspect_keywords):\n",
    "    \"\"\"\n",
    "    Extracts aspects based on predefined keywords from the given text.\n",
    "    Returns lines containing each aspect keyword.\n",
    "\n",
    "    :param text: Input text from which aspects are to be extracted\n",
    "    :param aspect_keywords: Dictionary of aspect keywords to look for\n",
    "    :return: Dictionary of found aspects with their occurrences\n",
    "    \"\"\"\n",
    "    found_aspects = {aspect: [] for aspect in aspect_keywords}\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line_lower = line.lower()\n",
    "        for aspect, keywords in aspect_keywords.items():\n",
    "            if any(keyword in line_lower for keyword in keywords):\n",
    "                found_aspects[aspect].append(line)\n",
    "    \n",
    "    return found_aspects\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"\"\"The service was ok!\n",
    "The food was good.\n",
    "However, the price was a bit high.\n",
    "The ambiance was excellent and the cleanliness was spot on.\"\"\"\n",
    "\n",
    "aspects = extract_aspects(sample_text, aspect_keywords)\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d589d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NITHIKESH\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: Food Quality\n",
      " - Sentiment: [{'label': '4 stars', 'score': 0.5057209730148315}]\n",
      "Aspect: Service\n",
      " - Sentiment: [{'label': '3 stars', 'score': 0.6658360362052917}]\n",
      "Aspect: Ambiance\n",
      " - Sentiment: [{'label': '5 stars', 'score': 0.6795399785041809}]\n",
      "Aspect: Pricing\n",
      " - Sentiment: [{'label': '3 stars', 'score': 0.6695561408996582}]\n",
      "Aspect: Cleanliness\n",
      " - Sentiment: [{'label': '5 stars', 'score': 0.6795399785041809}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def analyze_aspect_sentiments(aspects):\n",
    "    aspect_sentiments = {}\n",
    "    for aspect, snippets in aspects.items():\n",
    "        aspect_sentiments[aspect] = []\n",
    "        for snippet in snippets:\n",
    "            sentiment = sentiment_pipeline(snippet)\n",
    "            aspect_sentiments[aspect].append(sentiment)\n",
    "    return aspect_sentiments\n",
    "\n",
    "# Example usage\n",
    "aspect_sentiments = analyze_aspect_sentiments(aspects)\n",
    "\n",
    "for aspect, sentiments in aspect_sentiments.items():\n",
    "    print(f\"Aspect: {aspect}\")\n",
    "    for sentiment in sentiments:\n",
    "        print(f\" - Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6712c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-----------------+--------------------+------------+-------+--------------------+-------+-----------+\n",
      "|         business_id|             user_id|stars|             name|           full_text|Food Quality|Service|            Ambiance|Pricing|Cleanliness|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+------------+-------+--------------------+-------+-----------+\n",
      "|---kPU91CF4Lq2-Wl...|zmgsdGzOp08BWJZ2y...|  5.0|Frankie's Raw Bar|Awesome raw bar /...|          []|     []|                  []|     []|         []|\n",
      "|--LC8cIrALInl2vyo...|lVW0BD_ZkWVGDVwBu...|  4.0|   Studio G Salon|Kathy at Studio G...|          []|     []|[Kathy at Studio ...|     []|         []|\n",
      "+--------------------+--------------------+-----+-----------------+--------------------+------------+-------+--------------------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- Food Quality: string (nullable = true)\n",
      " |-- Service: string (nullable = true)\n",
      " |-- Ambiance: string (nullable = true)\n",
      " |-- Pricing: string (nullable = true)\n",
      " |-- Cleanliness: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType, StringType, ArrayType\n",
    "\n",
    "# Register UDF\n",
    "aspect_udf = udf(lambda text: extract_aspects(text, aspect_keywords), MapType(StringType(), StringType()))\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "df_with_aspects = df_grouped.withColumn(\"aspects\", aspect_udf(col(\"full_text\")))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# df_with_aspects.show(1)\n",
    "\n",
    "\n",
    "# Create separate columns for each aspect\n",
    "for aspect in aspect_keywords.keys():\n",
    "    df_with_aspects = df_with_aspects.withColumn(\n",
    "        aspect,\n",
    "        col(\"aspects\").getItem(aspect)\n",
    "    )\n",
    "\n",
    "# Drop the original 'aspects' column if no longer needed\n",
    "df_with_aspects = df_with_aspects.drop(\"aspects\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_with_aspects.show(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_with_aspects.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dd5a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+--------+--------+--------+-----------+\n",
      "|         business_id|             user_id|stars|                name|           full_text|Food Quality| Service|Ambiance| Pricing|Cleanliness|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+--------+--------+--------+-----------+\n",
      "|---kPU91CF4Lq2-Wl...|zmgsdGzOp08BWJZ2y...|  5.0|   Frankie's Raw Bar|Awesome raw bar /...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|--LC8cIrALInl2vyo...|lVW0BD_ZkWVGDVwBu...|  4.0|      Studio G Salon|Kathy at Studio G...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-02xFuruu85XmDn2x...|Pl6Gs5QXihWsUbcSN...|  5.0|Family Vision Center|This place is awe...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-0YvX6VK5S0u_hbQF...|T9tYtHUKNmJE1N6Ar...|  1.0| European Motorworks|Don't believe tho...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-0iIxySkp97WNlwK6...|cRANsQ5E_sxeQ0Aoo...|  4.0|Truckee Bagel Com...|I met a friend he...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-0qOecqGXEf_6Twai...|lDPJqWLoTiF9-DwH2...|  5.0|Sharmaine's Salon...|Sharmaine's just ...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-1XSzguS6XLN-V6MV...|qVFwj1GkZ2nMKNw8O...|  5.0|  Restaurant Rebirth|Are here twice be...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-1q3O9hvYmrELU39_...|u2X1zQkn_442L-V5x...|  1.0| Riley Bill Painting|Rude and confront...|    NEGATIVE|POSITIVE|POSITIVE|NEGATIVE|   POSITIVE|\n",
      "|-1zRvh3yjKAa2eYgg...|90be12_bmqOIBlJzO...|  1.0|        CVS Pharmacy|Incompetent. Rude...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-2gmbMDzKgYZ_8DOn...|nSB8UFphCkVrmRZwg...|  2.0|           New China|Food is mediocre ...|    NEGATIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-3e3CP3FFc-rvJj_-...|2-RlTk9dJNpj85MR2...|  2.0|      Penn's Landing|We left the gorge...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-4qgeqxb2hKe3dUUH...|3x-kbXQUhrsWOcp8m...|  5.0|     T&T Supermarket|I LOVE T&T!!! I l...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-5Xfp4_pojZjGgOev...|0J3kj11XGKu76cUx1...|  5.0|Algie Hailey's Ba...|Classic cuts at a...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-6vtoe3VFedMGouTF...|xLF_5_8FgH-AyHA7p...|  1.0|          AT&T Store|There are no word...|    POSITIVE|POSITIVE|POSITIVE|NEGATIVE|   POSITIVE|\n",
      "|-7qO7OvcQS7t6v6uH...|sVr6_v-lwixu3VREO...|  4.0|           Java Jive|4 stars to Java J...|    POSITIVE|POSITIVE|POSITIVE|NEGATIVE|   POSITIVE|\n",
      "|-7sP9Iyk6XfXRawkc...|jtCtMZFqPvY12Bv5k...|  1.0|  Dan's Paint & Body|Horrible communic...|    NEGATIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-81BXpO5Fuk-RqCab...|Cufcy7I9IZmLP0XhG...|  5.0|Big Storm Brewery...|This place is lik...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-8562lttAp_PuLWpQ...|QnYpGD92cAM_cnoGD...|  4.0|        Quinn's Pond|With a beautiful ...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-8VpP01AKfnt4wpT-...|Oi1qbcz2m2SnwUezt...|  4.0|     Boccella's Deli|These folks reall...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "|-9WXuHHFaFx5yGjkO...|ZVkXv5rRqlAuMWfx9...|  5.0|      Monsoon Vapors|Great place! Grea...|    POSITIVE|POSITIVE|POSITIVE|POSITIVE|   POSITIVE|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+------------+--------+--------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o198.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1280/834865936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Write the transformed DataFrame to a new file or replace the original\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\NITHIKESH\\Downloads\\extracted\\dataset\\output.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1721\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m     def text(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o198.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_label(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    try:\n",
    "        result = sentiment_pipeline(text)[0]\n",
    "        return result['label']\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Register the UDF in Spark\n",
    "sentiment_udf = udf(get_sentiment_label, StringType())\n",
    "\n",
    "# Load your data (replace with your actual data source)\n",
    "df = df_with_aspects\n",
    "\n",
    "# Apply the sentiment UDF to the specific columns\n",
    "\n",
    "df = df.withColumn(\"Food Quality\", sentiment_udf(df[\"Food Quality\"])) \\\n",
    "       .withColumn(\"Service\", sentiment_udf(df[\"Service\"])) \\\n",
    "       .withColumn(\"Ambiance\", sentiment_udf(df[\"Ambiance\"])) \\\n",
    "       .withColumn(\"Pricing\", sentiment_udf(df[\"Pricing\"])) \\\n",
    "       .withColumn(\"Cleanliness\", sentiment_udf(df[\"Cleanliness\"]))\n",
    "\n",
    "# Show the updated DataFrame with sentiment labels\n",
    "df.show()\n",
    "\n",
    "# Write the transformed DataFrame to a new file or replace the original\n",
    "# df.write.parquet(r\"C:\\Users\\NITHIKESH\\Downloads\\extracted\\dataset\\output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d878ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- Food Quality: string (nullable = true)\n",
      " |-- Service: string (nullable = true)\n",
      " |-- Ambiance: string (nullable = true)\n",
      " |-- Pricing: string (nullable = true)\n",
      " |-- Cleanliness: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1436f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
